A simple feedforward neural network library, based off of the code from Micheal Nielsen's excellent book "Neural Networks and Deep Learning".

Original code can be found here: https://github.com/unexploredtest/neural-networks-and-deep-learning

The code has been modified by adding ReLU and Leaky ReLU activation functions and their prime functions, He initialization, L2 regularization, learning rate decay, and functions for data augmentation.

I did this because I wanted to learn the underlying math and processes behind modern ML frameworks such as PyTorch and equally as important, it's cool. If you run the main function unmodified, the network should be able to hit 99% accuracy on MNIST after just a handful of epochs. See the below screenshot for proof. 

Definetly check out the book! It's an amazing intro to neural networks and really helps you form an intuitive understanding of how they work.


<img width="435" alt="Screenshot 2024-08-12 at 5 56 02â€¯PM" src="https://github.com/user-attachments/assets/6bc38237-ec7c-4d5b-8b6a-17823f198ae3">
